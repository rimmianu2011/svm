{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# SVM (Interview Level) — Student Lab\n",
      "\n",
      "We use sklearn here, but you still compute hinge loss and reason about margins." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 0 — Synthetic datasets\n",
      "We create 2D datasets for intuition." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def make_linear(n=300, sep=2.0):\n",
      "    X0 = rng.standard_normal((n//2, 2)) + np.array([0,0])\n",
      "    X1 = rng.standard_normal((n-n//2, 2)) + np.array([sep,0])\n",
      "    X = np.vstack([X0, X1])\n",
      "    y = np.array([0]*len(X0) + [1]*len(X1))\n",
      "    p = rng.permutation(n)\n",
      "    return X[p], y[p]\n",
      "\n",
      "def make_xor(n=400, noise=0.2):\n",
      "    X = rng.uniform(-1, 1, size=(n,2))\n",
      "    y = (X[:,0]*X[:,1] > 0).astype(int)\n",
      "    # add noise\n",
      "    flip = rng.random(n) < noise\n",
      "    y[flip] = 1 - y[flip]\n",
      "    return X, y\n",
      "\n",
      "X_lin, y_lin = make_linear()\n",
      "X_xor, y_xor = make_xor()\n",
      "X_lin.shape, X_xor.shape"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Hinge loss\n",
      "\n",
      "### Task 1.1: Implement hinge loss for y in {-1,+1}\n",
      "\n",
      "L = mean(max(0, 1 - y*(Xw + b)))\n",
      "\n",
      "# HINT: convert y from {0,1} to {-1,+1} as y2 = 2y-1" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def hinge_loss(X, y_pm1, w, b):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "X = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
      "y = np.array([1, 0])\n",
      "y_pm1 = 2*y - 1\n",
      "w = np.array([1.0, -1.0])\n",
      "b = 0.0\n",
      "L = hinge_loss(X, y_pm1, w, b)\n",
      "print('hinge', L)\n",
      "check('finite', np.isfinite(L))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Linear SVM (LinearSVC)\n",
      "\n",
      "### Task 2.1: Standardize + fit for different C\n",
      "\n",
      "**FAANG gotcha:** Without scaling, SVM behaves poorly." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def fit_linear_svm(X, y, C):\n",
      "    scaler = StandardScaler()\n",
      "    Xs = scaler.fit_transform(X)\n",
      "    clf = LinearSVC(C=C, max_iter=20000, random_state=0)\n",
      "    clf.fit(Xs, y)\n",
      "    return scaler, clf\n",
      "\n",
      "Cs = [0.01, 0.1, 1.0, 10.0]\n",
      "for C in Cs:\n",
      "    scaler, clf = fit_linear_svm(X_lin, y_lin, C)\n",
      "    acc = clf.score(scaler.transform(X_lin), y_lin)\n",
      "    print('C', C, 'train_acc', acc)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Kernel SVM (RBF)\n",
      "\n",
      "### Task 3.1: XOR dataset\n",
      "Train:\n",
      "- linear kernel (should struggle)\n",
      "- RBF kernel (should improve)" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "scaler = StandardScaler()\n",
      "Xx = scaler.fit_transform(X_xor)\n",
      "\n",
      "clf_lin = SVC(kernel='linear', C=1.0)\n",
      "clf_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
      "\n",
      "clf_lin.fit(Xx, y_xor)\n",
      "clf_rbf.fit(Xx, y_xor)\n",
      "\n",
      "print('linear acc', clf_lin.score(Xx, y_xor))\n",
      "print('rbf acc', clf_rbf.score(Xx, y_xor))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Failure modes\n",
      "\n",
      "### Task 4.1: Show scaling sensitivity\n",
      "Multiply one feature by 1000 and compare performance with/without StandardScaler." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "X_bad = X_lin.copy()\n",
      "X_bad[:, 1] *= 1000\n",
      "\n",
      "# without scaling\n",
      "clf = LinearSVC(C=1.0, max_iter=20000, random_state=0)\n",
      "clf.fit(X_bad, y_lin)\n",
      "acc_noscale = clf.score(X_bad, y_lin)\n",
      "\n",
      "# with scaling\n",
      "scaler = StandardScaler()\n",
      "Xs = scaler.fit_transform(X_bad)\n",
      "clf2 = LinearSVC(C=1.0, max_iter=20000, random_state=0)\n",
      "clf2.fit(Xs, y_lin)\n",
      "acc_scale = clf2.score(Xs, y_lin)\n",
      "\n",
      "print('acc_noscale', acc_noscale, 'acc_scale', acc_scale)\n",
      "check('scaling_helpful', acc_scale >= acc_noscale - 1e-9)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- Hinge loss implemented\n",
      "- C sweep shown\n",
      "- Linear vs RBF on XOR shown\n",
      "- Scaling failure demonstrated\n"
    ]}
  ]
}
